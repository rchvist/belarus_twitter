{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import yaml\n",
    "#\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import tqdm \n",
    "import os\n",
    "# from datetime import datetime, timezone, timedelta\n",
    "import re\n",
    "from twarc.client2 import Twarc2\n",
    "from twarc.expansions import ensure_flattened, flatten\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# import twarc_csv\n",
    "from twarc_csv import CSVConverter\n",
    "from v2_csv_converter import DataFrameConverter\n",
    "from rtweet_conversion import extra_cols, torename\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIGHT conversion columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols=['display_text_width',\n",
    "    'urls_url', # Normalement c'est la liste des URLS mentionnées, tronqué par rtweets. urls_expanded_url contient la 1ère (?) de ces URLS\n",
    "    'urls_t.co', \n",
    "    'geo_coords', # Vide dans Rtweets, on conserve \"coords_coords\"\n",
    "    'favorite_count', # Même chose que les likes, existe en double dans rtweets\n",
    "    'profile_banner_url', \n",
    "    'profile_background_url',\n",
    "    'media_expanded_url', # C'est le lien vers l'image dans le status, on peut le reconstruire si besoin. Dans la v2 pas d'expanded_url.\n",
    "    'media_t.co',\n",
    "    # Ext_media : Dans la v1, renvoie une liste si plusieurs medias (4 photos max)\n",
    "    # On conserve ext_media_url\n",
    "    'ext_media_expanded_url', \n",
    "    'ext_media_t.co',\n",
    "    'profile_url', # N'existe pas V2\n",
    "    'profile_expanded_url',\n",
    "    'account_lang', # Vide\n",
    "    'reply_to_screen_name', # Beaucoup de données manquantes, TODO les ré-imputer depuis le JSON ?\n",
    "    'profile_image_url', # Inutile\n",
    "    'media_url' # Redondant avec ext_media_url\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols=['display_text_width',\n",
    "    'urls_url', # Normalement c'est la liste des URLS mentionnées, tronqué par rtweets. urls_expanded_url contient la 1ère (?) de ces URLS\n",
    "    'urls_t.co', \n",
    "    'geo_coords', # Vide dans Rtweets, on conserve \"coords_coords\"\n",
    "    'favorite_count', # Même chose que les likes, existe en double dans rtweets\n",
    "    'profile_banner_url', \n",
    "    'profile_background_url',\n",
    "    'media_expanded_url', # C'est le lien vers l'image dans le status, on peut le reconstruire si besoin. Dans la v2 pas d'expanded_url.\n",
    "    'media_t.co',\n",
    "    # Ext_media : Dans la v1, renvoie une liste si plusieurs medias (4 photos max)\n",
    "    # On conserve ext_media_url\n",
    "    'ext_media_expanded_url', \n",
    "    'ext_media_t.co',\n",
    "    'profile_url', # N'existe pas V2\n",
    "    'profile_expanded_url',\n",
    "    'account_lang', # Vide\n",
    "    'reply_to_screen_name', # Beaucoup de données manquantes, TODO les ré-imputer depuis le JSON ?\n",
    "    'profile_image_url', # Inutile\n",
    "    'media_url', # Redondant avec ext_media_url\n",
    "    # many retweet fields are'nt useful since they inherit everything from the original tweet\n",
    "    # 'retweet_location',\n",
    "    # 'retweet_description',\n",
    "    'retweet_verified',\n",
    "    'retweet_text',\n",
    "    'retweet_source',\n",
    "    'retweet_retweet_count',\n",
    "    'retweet_favorite_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokeep columns : 77\n",
      "default columns : 112\n",
      "['id', 'referenced_tweets.replied_to.id', 'referenced_tweets.retweeted.id', 'in_reply_to_user_id', 'referenced_tweets.quoted.id', 'author_id', 'retweeted_user_id', 'quoted_user_id', 'created_at', 'text', 'lang', 'source', 'public_metrics.like_count', 'public_metrics.quote_count', 'public_metrics.reply_count', 'public_metrics.retweet_count', 'author.created_at', 'author.username', 'author.name', 'author.description', 'author.location', 'author.pinned_tweet_id', 'author.protected', 'author.public_metrics.followers_count', 'author.public_metrics.following_count', 'author.public_metrics.listed_count', 'author.public_metrics.tweet_count', 'author.url', 'author.verified', 'geo.coordinates.coordinates', 'geo.country', 'geo.country_code', 'geo.full_name', 'geo.geo.bbox', 'geo.name', 'geo.place_id', 'geo.place_type', 'entities.cashtags', 'is_retweet', 'is_quote', 'retweet_location', 'quoted_text', 'retweet_description', 'quoted_screen_name', 'retweet_source', 'quoted_name', 'retweet_statuses_count', 'quoted_verified', 'retweet_favorite_count', 'retweet_verified', 'quoted_created_at', 'quoted_description', 'retweet_screen_name', 'quoted_friends_count', 'retweet_friends_count', 'quoted_location', 'retweet_followers_count', 'media', 'ext_media_url', 'ext_media_type', 'quoted_retweet_count', 'quoted_source', 'quoted_followers_count', 'retweet_retweet_count', 'retweet_created_at', 'quoted_statuses_count', 'quoted_favorite_count', 'mentions_user_id', 'retweet_text', 'retweet_name', 'mentions_screen_name', 'hashtags', 'urls_expanded_url', 'ext_media_views', 'context_annotations', 'entities.annotations', 'geo.id']\n"
     ]
    }
   ],
   "source": [
    "from v2_csv_converter import DEFAULT_TWEET_COLUMNS\n",
    "# from rtweet_conversion import dropcols\n",
    "tokeep=[k for k,v in torename.items() if v not in dropcols]+extra_cols+['context_annotations','entities.annotations', 'geo.id']\n",
    "print('tokeep columns : %d'%len(tokeep))\n",
    "print('default columns : %d'%(len(DEFAULT_TWEET_COLUMNS)+len(extra_cols)+len(['context_annotations','entities.annotations', 'geo.id'])))\n",
    "print(tokeep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIGHT CONVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179ff4cceb314dc0a83ff9807fc4f2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/482M of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can also use :\n",
    "# [obj for obj in os.scandir(\"dir\") if obj.is_dir()] # obj.is_file(), obj.name\n",
    "# Process multiple json files to csv, Light version\n",
    "data=pd.DataFrame()\n",
    "# Merge JSON for data collection that was split ('query1.json', 'query2.json'...) \n",
    "for f in os.scandir('newdatacollection')  :\n",
    "    if 'Htags' in f.name :\n",
    "        converter_L = DataFrameConverter(\n",
    "        input_data_type=\"tweets\",\n",
    "        json_encode_all=False,\n",
    "        json_encode_text=False,\n",
    "        json_encode_lists=True,### TRUE ?\n",
    "        inline_referenced_tweets=False,\n",
    "        merge_retweets=True,\n",
    "        allow_duplicates=False,\n",
    "        extra_input_columns=','.join(extra_cols),\n",
    "        output_columns=','.join(tokeep)\n",
    "        )\n",
    "\n",
    "        with open(f, \"r\") as infile:\n",
    "            with open(\"newdatacollection/\"+ f.name.rsplit('.', 1)[0] + '.csv', \"w\") as outfile:\n",
    "                converter = CSVConverter(infile, outfile, converter=converter_L)\n",
    "                converter.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same process if you have jsonl files (1 json object per line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to 1608_2008_queries_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189e755e0ec94a058be061d3f4fc8e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/2.16G of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to 0820-0920_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1424a8a2b3c949d38a96a043bf97687e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/5.77G of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to 0920_1030_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4797b65bb1c8465294c9021076a9f58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/3.90G of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to 1030-1130_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2fc134c561470e92d0623ef13497c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/1.51G of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to 1130_1231_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a44b886a1de495aaca60f25e0d5a715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/916M of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to from_минске_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0767b2989b714fde8100c0229193f359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/635M of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to hashtags_to_minske_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b36f2e39da4171b4af855d3950fea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/1.09G of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to hashtags2_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f0b6768a2142cf82b540cbebf0667b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/740M of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to hashtags_trending_short_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1404b66685cf4a27838196df8bd68341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/388M of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Returns all filenames matching regex in dir and subdirs\n",
    "import glob\n",
    "from pathlib import Path\n",
    "def get_files_w_extension(base_dir, extension):\n",
    "    return glob.iglob(rf\"{base_dir}\\**\\*.{extension}\", recursive=True)\n",
    "# Process multiple JSON files to CSV  LIGHT\n",
    "data=pd.DataFrame()\n",
    "i=0\n",
    "for f in get_files_w_extension(\"MOBILISE_backup\\\\\", \"jsonl\") :\n",
    "    converter_L = DataFrameConverter(\n",
    "    input_data_type=\"tweets\",\n",
    "    json_encode_all=False,\n",
    "    json_encode_text=False,\n",
    "    json_encode_lists=True,\n",
    "    inline_referenced_tweets=False,\n",
    "    merge_retweets=True,\n",
    "    allow_duplicates=False,\n",
    "    extra_input_columns=','.join(extra_cols),\n",
    "    output_columns=','.join(tokeep)\n",
    ")\n",
    "    with open(f, \"r\") as infile:\n",
    "        o_name = infile.name.rsplit('.', 1)[0].rsplit('\\\\',1)[-1]+'.csv'\n",
    "        print('Converting file to '+o_name+' ...')\n",
    "        with open(\"export_folder/\"+o_name, \"w\") as outfile:\n",
    "            converter = CSVConverter(infile, outfile, converter=converter_L)\n",
    "            converter.process()\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGING CSVS\n",
    "Early batches of data were collected in the format 'data/query1/keywords0_to_1023.csv', 'data/query1/keywords1024_to_1300.csv'...   \n",
    "While we will eventually merge the entire data, we want to keep one file per query. To answer questions such as, how many tweets did each query yield ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '2020_0601_0808_g1q1.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-6cadaa39729c>:6: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '2020_0601_0808_g1q2.csv'>\n",
      "concatenating files to : newdatacollection2/query1.csv\n",
      "<DirEntry '2020_0601_0808_g2q1.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-6cadaa39729c>:6: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '2020_0601_0808_g2q2.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-6cadaa39729c>:6: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating files to : newdatacollection2/query2.csv\n",
      "<DirEntry '2020_0601_0808_g3q1.csv'>\n",
      "<DirEntry '2020_0601_0808_g3q2.csv'>\n",
      "concatenating files to : newdatacollection2/query3.csv\n",
      "<DirEntry '2020_0601_0808_g4q1.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-6cadaa39729c>:6: DtypeWarning: Columns (29,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating files to : newdatacollection2/query4.csv\n",
      "<DirEntry '2020_0601_0701_g5q1.csv'>\n",
      "<DirEntry '2020_0701_0715_g5q1.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-6cadaa39729c>:6: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '2020_0715_0731_g5q1.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-6cadaa39729c>:6: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '2020_0801_0808_g5q1.csv'>\n",
      "concatenating files to : newdatacollection2/query5.csv\n"
     ]
    }
   ],
   "source": [
    "to_merge = [f'data/newdata/query{n+1}' for n in range (5)]\n",
    "for d in to_merge :\n",
    "    data=pd.DataFrame()\n",
    "    for f in os.scandir(d) :\n",
    "        print(f)\n",
    "        data=pd.concat([data, pd.read_csv(f)])\n",
    "        data['id']=data['id'].astype(str)\n",
    "        data.drop_duplicates(subset='id', inplace=True, keep='first')\n",
    "    print('concatenating files to : '+d+'.csv')\n",
    "    data.to_csv(d+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry 'hashtags1_top100_a.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_11176\\2500193842.py:6: DtypeWarning: Columns (30,38,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry 'hashtags1_top100_b.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_11176\\2500193842.py:6: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating files to : data/newdata/hashtags1_top100.csv\n"
     ]
    }
   ],
   "source": [
    "to_merge = [f'data/newdata/hashtags1_top100']\n",
    "for d in to_merge :\n",
    "    data=pd.DataFrame()\n",
    "    for f in os.scandir(d) :\n",
    "        print(f)\n",
    "        data=pd.concat([data, pd.read_csv(f)])\n",
    "        data['id']=data['id'].astype(str)\n",
    "        data.drop_duplicates(subset='id', inplace=True, keep='first')\n",
    "    print('concatenating files to : '+d+'.csv')\n",
    "    data.to_csv('data/newdata/new_hashtags/hashtags_top100'+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge CSVs from subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (19,20,21,22,31,33,34,35,41,56,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (20,21,58,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (31,33,34,35,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (19,20,21,31,33,34,35,41,56,58,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (20,21,41,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (23,31,33,34,35,56,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (19,20,21,31,33,34,35,41,56,58,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (20,21,22,41,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (19,20,21,23,31,33,34,35,41,56,58,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (19,20,21,23,31,33,34,35,41,56,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (20,21,23,31,33,34,35,41,58,61,62,63,64,65,66,67,68,69) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (19,20,21,23,31,33,34,35,41,56,58,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (19,23,31,33,34,35,56) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (19,20,21,23,31,33,34,35,41,56,58,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (19,20,21,23,31,33,34,35,41,56,58,59,60,61,62,63,64,65,66,67,68,69) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n",
      "C:\\Users\\GGB\\AppData\\Local\\Temp\\ipykernel_19476\\3557953265.py:7: DtypeWarning: Columns (19,20,21,23,31,33,34,35,41,56,58,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.concat([df, pd.read_csv(f)])\n"
     ]
    }
   ],
   "source": [
    "# Merge multiple csv from subdirectories \n",
    "# such as ('subdir/keywords0_to_1023.csv', 'subdir/keywords1024_to_1300.csv') => subdir.csv\n",
    "subdirs = [obj for obj in os.scandir(\"newexport\") if obj.is_dir()] #may use obj.name\n",
    "for s in subdirs :\n",
    "    df = pd.DataFrame()\n",
    "    for f in [file for file in os.scandir(s.path) if '.csv' in file.name] :\n",
    "        df = pd.concat([df, pd.read_csv(f)])\n",
    "        df = df.drop_duplicates(subset='id', keep='first')\n",
    "    df.to_csv(s.path+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
