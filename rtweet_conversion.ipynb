{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import yaml\n",
    "#\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import tqdm \n",
    "import os\n",
    "# from datetime import datetime, timezone, timedelta\n",
    "import re\n",
    "from twarc.client2 import Twarc2\n",
    "from twarc.expansions import ensure_flattened, flatten\n",
    "from rtweet_conversion import extra_cols, torename\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# import twarc_csv\n",
    "from twarc_csv import CSVConverter\n",
    "from v2_csv_converter import DataFrameConverter\n",
    "from rtweet_conversion import extra_cols, torename\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process json files \n",
    "to V1-friendly CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=converter_R.process(ensure_flattened(ex_dataset))\n",
    "# df=converter_M.process(ensure_flattened(ex_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rtweet_conversion import dropcols_v1, torename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author.entities.url.urls', 'author.profile_image_url']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k, v in torename.items() if v in dropcols_v1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fields we ended up removing from Rtweet after analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols=['display_text_width',\n",
    "    'urls_url', # Normalement c'est la liste des URLS mentionnées, tronqué par rtweets. urls_expanded_url contient la 1ère (?) de ces URLS\n",
    "    'urls_t.co', \n",
    "    'geo_coords', # Vide dans Rtweets, on conserve \"coords_coords\"\n",
    "    'favorite_count', # Même chose que les likes, existe en double dans rtweets\n",
    "    'profile_banner_url', \n",
    "    'profile_background_url',\n",
    "    'media_expanded_url', # C'est le lien vers l'image dans le status, on peut le reconstruire si besoin. Dans la v2 pas d'expanded_url.\n",
    "    'media_t.co',\n",
    "    # Ext_media : Dans la v1, renvoie une liste si plusieurs medias (4 photos max)\n",
    "    # On conserve ext_media_url\n",
    "    'ext_media_expanded_url', \n",
    "    'ext_media_t.co',\n",
    "    'profile_url', # N'existe pas V2\n",
    "    'profile_expanded_url',\n",
    "    'account_lang', # Vide\n",
    "    'reply_to_screen_name', # Beaucoup de données manquantes, TODO les ré-imputer depuis le JSON ?\n",
    "    'profile_image_url', # Inutile\n",
    "    'media_url', # Redondant avec ext_media_url\n",
    "    # many retweet fields are'nt useful since they inherit everything from the original tweet\n",
    "    # 'retweet_location',\n",
    "    # 'retweet_description',\n",
    "    'retweet_verified',\n",
    "    'retweet_text',\n",
    "    'retweet_source',\n",
    "    'retweet_retweet_count',\n",
    "    'retweet_favorite_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want to extract what we need (Rtweet fields) from the JSON, plus a few features from V2 API that might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokeep columns : 77\n",
      "default columns : 112\n",
      "['id', 'referenced_tweets.replied_to.id', 'referenced_tweets.retweeted.id', 'in_reply_to_user_id', 'referenced_tweets.quoted.id', 'author_id', 'retweeted_user_id', 'quoted_user_id', 'created_at', 'text', 'lang', 'source', 'public_metrics.like_count', 'public_metrics.quote_count', 'public_metrics.reply_count', 'public_metrics.retweet_count', 'author.created_at', 'author.username', 'author.name', 'author.description', 'author.location', 'author.pinned_tweet_id', 'author.protected', 'author.public_metrics.followers_count', 'author.public_metrics.following_count', 'author.public_metrics.listed_count', 'author.public_metrics.tweet_count', 'author.url', 'author.verified', 'geo.coordinates.coordinates', 'geo.country', 'geo.country_code', 'geo.full_name', 'geo.geo.bbox', 'geo.name', 'geo.place_id', 'geo.place_type', 'entities.cashtags', 'is_retweet', 'is_quote', 'retweet_location', 'quoted_text', 'retweet_description', 'quoted_screen_name', 'retweet_source', 'quoted_name', 'retweet_statuses_count', 'quoted_verified', 'retweet_favorite_count', 'retweet_verified', 'quoted_created_at', 'quoted_description', 'retweet_screen_name', 'quoted_friends_count', 'retweet_friends_count', 'quoted_location', 'retweet_followers_count', 'media', 'ext_media_url', 'ext_media_type', 'quoted_retweet_count', 'quoted_source', 'quoted_followers_count', 'retweet_retweet_count', 'retweet_created_at', 'quoted_statuses_count', 'quoted_favorite_count', 'mentions_user_id', 'retweet_text', 'retweet_name', 'mentions_screen_name', 'hashtags', 'urls_expanded_url', 'ext_media_views', 'context_annotations', 'entities.annotations', 'geo.id']\n"
     ]
    }
   ],
   "source": [
    "from v2_csv_converter import DEFAULT_TWEET_COLUMNS\n",
    "tokeep=[k for k,v in torename.items() if v not in dropcols]+extra_cols+['context_annotations','entities.annotations', 'geo.id']\n",
    "print('tokeep columns : %d'%len(tokeep))\n",
    "print('default columns : %d'%(len(DEFAULT_TWEET_COLUMNS)+len(extra_cols)+len(['context_annotations','entities.annotations', 'geo.id'])))\n",
    "print(tokeep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179ff4cceb314dc0a83ff9807fc4f2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/482M of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can also use :\n",
    "# [obj for obj in os.scandir(\"newexport\") if obj.is_dir()] # obj.is_file(), obj.name\n",
    "# Process multiple json files to csv, Light version\n",
    "data=pd.DataFrame()\n",
    "# Merge JSON for data collection that was split ('query1', 'query2'...) \n",
    "for f in [f for f in os.scandir('datasets/new_data_collection')]  :\n",
    "    if 'Htags' in f.name :\n",
    "        converter_L = DataFrameConverter(\n",
    "        input_data_type=\"tweets\",\n",
    "        json_encode_all=False,\n",
    "        json_encode_text=False,\n",
    "        json_encode_lists=True,### TRUE ?\n",
    "        inline_referenced_tweets=False,\n",
    "        merge_retweets=True,\n",
    "        allow_duplicates=False,\n",
    "        extra_input_columns=','.join(extra_cols),\n",
    "        output_columns=','.join(tokeep)\n",
    "        )\n",
    "        # if 'query' in f.name :\n",
    "        #     with open(f, \"r\") as infile:\n",
    "        #         outpath = \"newexport_light/\"+f.name.split('_query', 1)[0]+ '/' + f.name.rsplit('.', 1)[0] + '_light.csv'\n",
    "        #         with open(outpath, \"w\") as outfile:\n",
    "        #             converter = CSVConverter(infile, outfile, converter=converter_L)\n",
    "        #             converter.process()\n",
    "\n",
    "        with open(f, \"r\") as infile:\n",
    "            with open(\"datasets/new_data_collection/\"+ f.name.rsplit('.', 1)[0] + '.csv', \"w\") as outfile:\n",
    "                converter = CSVConverter(infile, outfile, converter=converter_L)\n",
    "                converter.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSONL (Inline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to 1608_2008_queries_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189e755e0ec94a058be061d3f4fc8e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/2.16G of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to 0820-0920_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1424a8a2b3c949d38a96a043bf97687e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/5.77G of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to 0920_1030_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4797b65bb1c8465294c9021076a9f58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/3.90G of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to 1030-1130_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2fc134c561470e92d0623ef13497c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/1.51G of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to 1130_1231_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a44b886a1de495aaca60f25e0d5a715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/916M of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to from_минске_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0767b2989b714fde8100c0229193f359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/635M of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to hashtags_to_minske_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b36f2e39da4171b4af855d3950fea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/1.09G of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to hashtags2_tweets_inline_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f0b6768a2142cf82b540cbebf0667b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/740M of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file to hashtags_trending_short_light.csv ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1404b66685cf4a27838196df8bd68341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Processed 0.00/388M of input file [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# JSONL\n",
    "# Returns all filenames matching regex in dir and subdirs\n",
    "import glob\n",
    "from pathlib import Path\n",
    "def get_jsonl_files(base_dir):\n",
    "    return glob.iglob(rf\"{base_dir}\\**\\*.jsonl\", recursive=True)\n",
    "# Process multiple JSON files to CSV  LIGHT\n",
    "data=pd.DataFrame()\n",
    "i=0\n",
    "for f in get_jsonl_files(\"D:\\\\MOBILISE_backup\\\\\") :\n",
    "    converter_L = DataFrameConverter(\n",
    "    input_data_type=\"tweets\",\n",
    "    json_encode_all=False,\n",
    "    json_encode_text=False,\n",
    "    json_encode_lists=True,\n",
    "    inline_referenced_tweets=False,\n",
    "    merge_retweets=True,\n",
    "    allow_duplicates=False,\n",
    "    extra_input_columns=','.join(extra_cols),\n",
    "    output_columns=','.join(tokeep)\n",
    ")\n",
    "    with open(f, \"r\") as infile:\n",
    "        o_name = infile.name.rsplit('.', 1)[0].rsplit('\\\\',1)[-1]+'_light.csv'\n",
    "        print('Converting file to '+o_name+' ...')\n",
    "        with open(\"newexport_light/\"+o_name, \"w\") as outfile:\n",
    "            converter = CSVConverter(infile, outfile, converter=converter_L)\n",
    "            converter.process()\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MERGING CSVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '2020_0601_0808_g1q1.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-6cadaa39729c>:6: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '2020_0601_0808_g1q2.csv'>\n",
      "concatenating files to : newdatacollection2/query1.csv\n",
      "<DirEntry '2020_0601_0808_g2q1.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-6cadaa39729c>:6: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '2020_0601_0808_g2q2.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-6cadaa39729c>:6: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating files to : newdatacollection2/query2.csv\n",
      "<DirEntry '2020_0601_0808_g3q1.csv'>\n",
      "<DirEntry '2020_0601_0808_g3q2.csv'>\n",
      "concatenating files to : newdatacollection2/query3.csv\n",
      "<DirEntry '2020_0601_0808_g4q1.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-6cadaa39729c>:6: DtypeWarning: Columns (29,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating files to : newdatacollection2/query4.csv\n",
      "<DirEntry '2020_0601_0701_g5q1.csv'>\n",
      "<DirEntry '2020_0701_0715_g5q1.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-6cadaa39729c>:6: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '2020_0715_0731_g5q1.csv'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-6cadaa39729c>:6: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data=pd.concat([data, pd.read_csv(f)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '2020_0801_0808_g5q1.csv'>\n",
      "concatenating files to : newdatacollection2/query5.csv\n"
     ]
    }
   ],
   "source": [
    "to_merge = [f'newdatacollection2/query{n+1}' for n in range (5)]\n",
    "for d in to_merge :\n",
    "    data=pd.DataFrame()\n",
    "    for f in os.scandir(d) :\n",
    "        print(f)\n",
    "        data=pd.concat([data, pd.read_csv(f)])\n",
    "        data['id']=data['id'].astype(str)\n",
    "        data.drop_duplicates(subset='id', inplace=True)\n",
    "    print('concatenating files to : '+d+'.csv')\n",
    "    data.to_csv(d+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple JSON conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple json files to csv\n",
    "for f in [f for f in os.scandir('D:\\MOBILISE_backup\\json')] : # if \"hashtags3\" in f.name\n",
    "    converter_R = DataFrameConverter(\n",
    "    input_data_type=\"tweets\",\n",
    "    json_encode_all=False,\n",
    "    json_encode_text=False,\n",
    "    json_encode_lists=True,\n",
    "    inline_referenced_tweets=False,\n",
    "    merge_retweets=True,\n",
    "    allow_duplicates=False,\n",
    "    extra_input_columns=','.join(extra_cols)\n",
    ")\n",
    "\n",
    "    with open(f, \"r\") as infile:\n",
    "        with open(\"newexport/\"+f.name.rsplit('.', 1)[0]+'.csv', \"w\") as outfile:\n",
    "            converter = CSVConverter(infile, outfile, converter=converter_R)\n",
    "            converter.process()\n",
    "#         data=pd.concat([data, pd.read_csv(\"newcols/\"+f.name.rsplit('.', 1)[0]+'.csv')])\n",
    "#         data['id']=data['id'].astype(str)\n",
    "#         data.drop_duplicates(subset='id', inplace=True)\n",
    "# data.set_index('id').to_csv('newexport/newcols_1.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple JSONLine conversion \n",
    "* Reading multiple subdirectories (glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\MOBILISE_backup\\0816-0820\\1608_2008_queries_tweets_inline.jsonl 1608_2008_queries_tweets_inline.csv\n",
      "D:\\MOBILISE_backup\\0820-0920\\0820-0920_tweets_inline.jsonl 0820-0920_tweets_inline.csv\n",
      "D:\\MOBILISE_backup\\0920-1030\\0920_1030_tweets_inline.jsonl 0920_1030_tweets_inline.csv\n",
      "D:\\MOBILISE_backup\\1030-1130\\1030-1130_tweets_inline.jsonl 1030-1130_tweets_inline.csv\n",
      "D:\\MOBILISE_backup\\Hashtags1\\from_минске_tweets_inline.jsonl from_минске_tweets_inline.csv\n",
      "D:\\MOBILISE_backup\\Hashtags2\\hashtags2_tweets_inline.jsonl hashtags2_tweets_inline.csv\n"
     ]
    }
   ],
   "source": [
    "for f in get_jsonl_files(\"D:\\MOBILISE_backup\") :\n",
    "    with open(f, \"r\") as infile:\n",
    "        print(f, infile.name.rsplit('.', 1)[0].rsplit('\\\\',-1)[-1]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns all filenames matching regex in dir and subdirs\n",
    "import glob\n",
    "from pathlib import Path\n",
    "def get_jsonl_files(base_dir):\n",
    "    return glob.iglob(rf\"{base_dir}\\**\\*.jsonl\", recursive=True)\n",
    "\n",
    "# We can also use :\n",
    "# [obj for obj in os.scandir(\"newexport\") if obj.is_dir()] # obj.is_file(), obj.name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
